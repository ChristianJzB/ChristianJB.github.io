<!DOCTYPE html>
<html>

    <head>
        <title>Christian Jimenez Beltran</title>
        <link rel="stylesheet" href="../css/format_ISR.css">

        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
        </script>   
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
            });
          </script>
              
          <script type="text/javascript"
                  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
          </script>

    </head>


    <body>
        <header>
            <img src="../images/logo.jpg" class="logo-img" >
            <nav>
                <ul>
                    <li><a href="../Index.html#home">Home</a></li>
                    <li><a href="../Index.html#ri">CV</a></li>
                    <li><a href="#isr">ISR</a></li>
                    <li><a href="../sec/OI.html">Other</a></li>
                </ul>
            </nav>

        </header>
        <main>
            <section id="isr">
                <div class="section-inner">
                    <h2>Bayesian neural networks and Gaussian Process</h2>
                    <h3>Report on the Gaussian Process and Uncertainty Summer School Seminar given by Yingzhen Li, Sep. 10, 2019: </h3>
                    <h3>Bayesian neural networks: a function space view tour</h3>

                    <div class="toc_container">
                        <p class="toc_title" >Contents</p>
                        <ul class="toc_list" style="list-style: none;">
                            <li><a href="#intro">1 Introduction</a></li>
                            <li><a href="#1">2 Mean-Field Bayesian Neural Networks and Gaussian Process</a></li>
                            <li><a href="#2">3 Inference with Gaussian Process and Bayesian Neural Networks</a></li>
                            <li><a href="#3">4 Bayesian Neural Networks function-space inference</a></li>
                            <li><a href="#4">5 Conclusion</a></li>
                            <li><a href="#ap">6 Appendix</a></li>
                            <li><a href="#ref">7 References</a></li>
                        </ul>
                    </div>



                    <h5 id="intro">Introduction</h5>

                    Neural networks (NN) are powerful algorithms that attempt to discern patterns or relationships in a data set. Over the last years, other kinds of NNs have arisen to improve some features of them or tackle particular problems. Bayesian neural networks (BNN) are special NNs that use Bayesian Inference to model data sets and, unlike NN, a BNN captures uncertainty. In the same way, a Gaussian process (GP) is another mathematical tool that is used to model a wide variety of phenomena. In the seminar delivered by Yingzhen Li, the connection between BNN with one hidden layer and GP is explored. Li discussed three important results: (i)Mean- Field BNN presents GP limits, (ii) link between BNN and approximate inference on GP's, and (iii) approximate inference on BNN's can leverage GP techniques. 
                    In the following sections, I present the results of the seminar.

                    <h5 id="1">Mean-Field Bayesian Neural Networks and Gaussian Process</h5>
                    Let us consider a BNN with one hidden layer and $M$ nodes,
    
                    \[ f(x) = \sum_{m = 1}^{M}v_{m}\phi (W_{m}^{T}x + b_{m}),\]

                    where $W_{1}=(W_1,...,W_m)^{T}$, $b=(b_1,...,b_m)^{T}$, $W_{2}=(v_1,...,v_2)^{T}$ and $\phi(.)$ are the weights and bias of the input layer, weights of the output layer and the activation function, respectively. In this framework, the parameters are treated as random variables and a mean-field prior is set up to them, then, 
    
                    \[\rho(W_{1})= \prod_{m} \rho(W_{m}), \quad  \rho(b)= \prod_{m} \rho(b_{m}), \quad \rho(W_{2})= \prod_{m} \rho(v_{m}), \]
    
                    where $\rho (w_{i})=\rho (w_{j}), \quad \rho (b_{i})=\rho (b_{j}), \quad \rho (v_{i})=\rho (v_{j}), \quad \forall i,j$.

                    Notice that, $h_{i}=\phi (W_{i}^{T}x + b_{i})$ and $h_{j}=\phi (W_{j}^{T}x + b_{j})$ are independent and identically distributed $,\forall i\not= j$. In consequence, $f(x)$ have the following form.
     
                    \[ f(x) = \sum_{m=1}^{M}v_{m}h_{m}.\]  
    
                    The last equation shows that $f(x)$ is the sum of $iid$ random variables. Consider that $\mathbb{E}[v_{i}]=0$ and $\mathbb{V}[v_{i}]= \sigma_{v}^{2}$. Using central theorem, we have that,
    
                    \[ \mathbb{E}[f(x)] = \sum_{m=1}^{M}\mathbb{E}[v_{m}]\mathbb{E}[h_{m}(x)]=0, \quad \mathbb{V}[f(x)] = \sum_{m=1}^{M}\sigma_{v}^{2}\mathbb{E}[h_{m}^{2}(x)] \longmapsto \sigma_{v}^{2}\mathbb{E}[h_{m}^{2}(x)], \] 
    
    
                    \[\mathrm{Cov}[f(x)f(x')] = \sum_{m=1}^{M}\sigma_{v}^{2}\mathbb{E}[h_{m}^{2}(x)h^{2}(x')] \longmapsto \sigma_{v}^{2}\mathbb{E}[h(x)h(x')].\] 
    
                    The equations above hold for any $x, x'$; hence, $f(x) \sim GP(0, K(x,x'))$, where $K(x,x')=\sigma_{v}^{2}\mathbb{E}[h(x)h(x')]$. Refer to <a href="#a">[1]</a> for more details of this section. <a href="#b">[2]</a> extends this result to Convolutional Neural Networks.
                    
                    <h5 id="2">Inference with Gaussian Process and Bayesian Neural Networks</h5>

                    In this work, we analyze the Bayesian regression employing Gaussian process as priors. The relationship between a dependent and an independent variable in a Bayesian regression has the following form:
    
                    \[f_{\star} | x_{\star}, X,y \sim \mathcal{N} \left( K_{\star n}(K_{nn}+\sigma^{2}I)^{-1}y,K_{\star\star}-K_{\star n} (K_{nn}+\sigma^{2}I)^{-1} K_{\star n} \right).\] 
    
                    where $x_{\star}$, $X$, and $y$ are the new data of the independent variable, the first set of observations of the independent data, and the data of the dependent variable, respectively. Additionally, $K_{ij}= k(x_{i},x_{j})$ is the kernel trick and $\sigma^{2}$ the variance of the error. 
    
                    The fundamental problem of the previous equation is that computing $(K_{nn}+\sigma^{2}I)^{-1}$ has a $\mathcal{O}(N^{3})$ cost; then, methods for tackling that problem have been developed. Let us introduce Bochener's theorem which uses the theory of Fourier transforms. 
    
    
                    Theorem
    
                    A (properly scaled) translation invariant kernel $K(x,x')=K(x-x')$ can be represented as
                    \[K(x,x')= \mathbb{E}_{\rho(w)\rho(b)}[\sigma^{2} \mathrm{ cos}(w^{T}x+b)\mathrm{ cos}(w^{T}x'+b)],\]
    
                    for some distribution $\rho(w)$ and $\rho(b)=Uniform[0,2\pi].$
    
                    I used a result from <a href="#c">[3]</a>. It is well known that, in Bayesian theory, Monte Carlo methods are used for estimating posterior distributions or other statistics. This work is not the exception. Therefore, the kernel is approximated by $\hat{K}(x,x')=\frac{\sigma^{2}}{M}h(x)^{T}h(x')$, where $M$ is the number of simulations and $h_{m}(x) = cos(w_{m}^{T}x+b_{m})$, $w_{m}^{T}\sim \rho(w),b_{m}\sim \rho(b)$. In consequence, the regression becomes:
    
                    \[\hat{f} \sim GP(0,\hat{K}(x,x')) \Leftrightarrow \hat{f}(x)=v^{T}h(x) \quad v \sim \rho (v)=\mathcal{N}(0,\frac{\sigma^{2}}{M} I).\]
    
                    Then, the Gaussian regression is equivalent to a BNN with one hidden layer. If M is increased, hidden units are added. Other works have tried to generalize this result. In particular, <a href="#d">[4]</a> presents the connection between deep Gaussian Process and deep BNN with bottleneck layers.
                    
                    <h5 id="3">Bayesian Neural Networks function-space inference</h5>

                    Making inferences in the weight space could present some issues when making inference. <a href="#e">[5]</a> discusses some of them and proposes a novel method to avoid such problems. This section explores the notion of function space and its impact on BNNs using implicit stochastic processes which arise from applying a function $f(.)$ to a set of random variables ${x}$. A formal definition of implicit stochastic process is presented in the appendix. 
    
                    A implicit process regression model has the following form:
                    \[f(.) \sim IP(g_{\theta}(.,.),\rho_{z}), \quad y=f(x)+\epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma^{2}).\]
    
                    As in the $GP$ $regression$, a Bayesian approach is used here also, so one wants to compute $p(f|x,y)$. By computing $p(f|x,y)$, one can build the posterior predictive distribution $p(y_{\star}|x_{\star},x,y)$. Although this is a Bayesian framework, the usual methods for estimating the posterior are not efficient in this approach.
    
                    To tackle this problem, the sleep and wake phases are used <a href="#f">[6]</a>. The idea behind this method is to approximate the implicit stochastic regression to a GP regression problem. As a result, we set the following equations: 
    
                    \[\mathrm{log} (q_{GP}^{\star}(y|x,\theta)) \approx \mathrm{log} ( \int \prod_{n}q^{\star}(y_{n}|x_{n},a,\theta)\rho (a)da),\]
    
                    \[q^{\star}(y_{n}|x_{n},a,\theta)= \mathcal{N}(y_{n}; m_{MLE}^{\star}(x_{n})+\frac{1}{\sqrt{S}}\sum_{m}\delta_{s}(x_{n})a_{s},\sigma^2), \quad \rho(a)=\mathcal{N}(a;0,I). \]
    
                    This approach has a better estimate of the mean and uncertainty of a data set as Li presented some examples in the seminar. 
                    
                    <h5 id="4">Conclusion</h5>

                    The seminar presented three distinct problems to verify the connection between GPs and BNN. Thanks to this link, scientists can solve fresh problems by transforming either BNN to a GP or GP to a BBN. I believe that this connection will become clearer and stronger and will help to tackle further issues in these fields.
                
                    <h5 id="ap">Appendix</h5>

                    Definition: An implicit stochastic process $(IP)$ is a collection of random variables $f(.)$ such that any finite collection $f=(f(x_{1}),...,f(x_{N}))^{T}$ has joint distribution implicitly defined by the following generative process:

                    \[z \sim \rho(z), \quad f(x_{n})=g_{\theta}(x_{n},z), \quad \forall x_{n} \in X \]

                    A function distributed according to the above $IP$ is denoted as $f(.) \sim IP(g_{\theta}(.,.),\rho_{z}).$

                    <h5 id="ref">References</h5>

                    <div class="boxhead">
                    <ol class="references"  >

                        <li id="a"><a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0" target="_blank">Radford M, Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996.</a></li>
                        <li id = "b"><a href="https://arxiv.org/abs/2101.04097"target="_blank">Adrià Garriga-Alonso and Mark van der Wilk. Correlated weights in infi-
                            nite limits of deep convolutional neural networks. In Cassio de Campos and
                            Marloes H. Maathuis, editors, Proceedings of the Thirty-Seventh Conference
                            on Uncertainty in Artificial Intelligence, volume 161 of Proceedings of Machine
                            Learning Research, pages 1998 to 2007. PMLR, 27 to 30 Jul 2021.</a></li>
                        <li id = "c"><a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf" target="_blank">Ali Rahimi and Benjamin Recht. Random features for large-scale kernel
                            machines. In Proceedings of the 20th International Conference on Neural Infor-
                            mation Processing Systems, NIPS’07, page 1177–1184, Red Hook, NY, USA,
                            2007. Curran Associates Inc.</a></li>
                        <li id = "d"><a href="https://arxiv.org/abs/1602.04133"target="_blank">Thang Bui, Daniel Hernandez-Lobato, Jose Hernandez-Lobato, Yingzhen
                            Li, and Richard Turner. Deep gaussian processes for regression using ap-
                            proximate expectation propagation. In Maria Florina Balcan and Kilian Q.
                            Weinberger, editors, Proceedings of The 33rd International Conference on Ma-
                            chine Learning, volume 48 of Proceedings of Machine Learning Research, pages
                            1472–1481, New York, New York, USA, 20–22 Jun 2016. PMLR.</a></li>
                        <li id = "e"><a href="https://arxiv.org/abs/1806.02390" target="_blank">Chao Ma, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Variational
                            implicit processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
                            editors, Proceedings of the 36th International Conference on Machine Learning,
                            volume 97 of Proceedings of Machine Learning Research, pages 4222–4233.
                            PMLR, 09–15 Jun 2019.</a></li>
                        <li id = "f"><a href="http://www.gatsby.ucl.ac.uk/~dayan/papers/hm95.pdf" target="_blank">Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel.
                            The Helmholtz Machine. Neural Computation, 7(5):889–904, 09 1995.</a></li>
                    </ol>
                   </div>
                </div>
            
            </section>

        </main>
        <footer>
            © Copyright Christian JB, 2022
        </footer>
    </body>







</html>